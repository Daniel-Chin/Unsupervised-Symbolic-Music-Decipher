{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import librosa\n",
    "\n",
    "from shared import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_PATH = './data/hum.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playHard(data):\n",
    "    return Audio(data, rate = ENCODEC_SR)\n",
    "def play(data, soft = .1):\n",
    "    t = np.concatenate([data, [1]])\n",
    "    length = round(soft * ENCODEC_SR)\n",
    "    t[:length ] = np.multiply(t[:length ], np.linspace(0, 1, length))\n",
    "    t[-length:] = np.multiply(t[-length:], np.linspace(1, 0, length))\n",
    "    return playHard(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_np, sr = librosa.load(PROMPT_PATH, sr=ENCODEC_SR)\n",
    "assert sr == ENCODEC_SR\n",
    "wave = torch.Tensor(wave_np).to(DEVICE)\n",
    "wave.shape, wave.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "from audiocraft.models.musicgen import MusicGen\n",
    "from audiocraft.models.multibanddiffusion import MultiBandDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DIFFUSION_DECODER = False\n",
    "musicGen = MusicGen.get_pretrained('facebook/musicgen-small', device='cuda')\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    mbd = MultiBandDiffusion.get_mbd_musicgen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musicGen.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=250,\n",
    "    duration=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodec = musicGen.compression_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    codes, _ = encodec.encode(wave.unsqueeze(0).unsqueeze(0))\n",
    "    recon: Tensor = encodec.decode(codes)[0, 0, :]   # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(recon.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_SR = 32000\n",
    "assert ENCODEC_SR == TEMP_SR\n",
    "def get_bip_bip(bip_duration=0.125, frequency=440,\n",
    "                duration=0.5, sample_rate=TEMP_SR, device=\"cuda\"):\n",
    "    \"\"\"Generates a series of bip bip at the given frequency.\"\"\"\n",
    "    t = torch.arange(\n",
    "        int(duration * sample_rate), device=\"cuda\", dtype=torch.float) / sample_rate\n",
    "    wav = torch.cos(2 * math.pi * 440 * t)[None]\n",
    "    tp = (t % (2 * bip_duration)) / (2 * bip_duration)\n",
    "    envelope = (tp >= 0.5).float()\n",
    "    return wav * envelope\n",
    "bipbip = get_bip_bip().cpu().numpy()[0, :]\n",
    "play(bipbip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = wave.unsqueeze(0).expand(1, -1, -1).to(DEVICE)\n",
    "prompt.shape, prompt.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = musicGen.generate_continuation(\n",
    "#     prompt, \n",
    "#     32000, \n",
    "#     [\n",
    "#         None, \n",
    "#         # 'Random dude humming jazz', \n",
    "#         # 'Heartful EDM with beautiful synths and chords', \n",
    "#     ], \n",
    "#     progress=True, \n",
    "# )\n",
    "# display_audio(res, 32000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.modules.conditioners import ClassifierFreeGuidanceDropout\n",
    "from audiocraft.solvers.musicgen import MusicGenSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes, prompt_tokens = musicGen._prepare_tokens_and_attributes([None], prompt[:1, :, :])\n",
    "assert prompt_tokens is not None\n",
    "print(attributes)\n",
    "print(prompt_tokens.shape)\n",
    "assert (prompt_tokens == codes).all().item()\n",
    "assert prompt_tokens.dtype == codes.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with musicGen.autocast:\n",
    "    lm = musicGen.lm\n",
    "    null_conditions = ClassifierFreeGuidanceDropout(p=1.0)(attributes)\n",
    "    print(attributes)\n",
    "    print(null_conditions)\n",
    "    conditions = attributes + null_conditions\n",
    "    tokenized = lm.condition_provider.tokenize(conditions)\n",
    "    cfg_conditions = lm.condition_provider(tokenized)\n",
    "    print(cfg_conditions)\n",
    "[x.dtype for x in cfg_conditions['description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, K, T = codes.shape\n",
    "start_offset = T\n",
    "unknown_token = -1\n",
    "max_gen_len = 1500\n",
    "gen_codes = torch.full((B, K, max_gen_len), unknown_token, dtype=torch.long, device=DEVICE)\n",
    "gen_codes[..., :start_offset] = codes\n",
    "pattern = lm.pattern_provider.get_pattern(max_gen_len)\n",
    "gen_sequence, indexes, mask = pattern.build_pattern_sequence(gen_codes, lm.special_token_id)\n",
    "gen_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_offset_sequence = pattern.get_first_step_with_timesteps(start_offset)\n",
    "print(f'{start_offset_sequence = }')\n",
    "with musicGen.autocast:\n",
    "    curr_sequence = gen_sequence[..., :start_offset_sequence]\n",
    "    curr_mask = mask[None, ..., :start_offset_sequence].expand(B, -1, -1)\n",
    "\n",
    "    # check coherence between mask and sequence\n",
    "    assert (curr_sequence == torch.where(curr_mask, curr_sequence, lm.special_token_id)).all()\n",
    "    # should never happen as gen_sequence is filled progressively\n",
    "    assert not (curr_sequence == unknown_token).any()\n",
    "\n",
    "    db_sequence = torch.cat([curr_sequence, curr_sequence], dim=0)\n",
    "    print(db_sequence.shape, db_sequence.dtype)\n",
    "    out = lm.forward(db_sequence, [], condition_tensors=cfg_conditions)\n",
    "    print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_neural_avh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
